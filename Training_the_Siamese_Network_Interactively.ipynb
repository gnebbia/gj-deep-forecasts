{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GJ Project: Training the Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from scipy.spatial import distance\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "import gjnn.model\n",
    "import gjnn.loss\n",
    "import gjnn.dataloader\n",
    "\n",
    "\n",
    "# Dataset Loading \n",
    "dataset = pd.read_csv(\"data/ds_medium.csv\", sep=None, engine='python',  dtype={'user_id_1': \"category\", \"user_id_2\":\"category\"})\n",
    "\n",
    "\n",
    "dataset.drop([\"ifp_id\"], axis =1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.head()\n",
    "\n",
    "# To modify when dataset column order will change\n",
    "features_user_1 = [0,1,2,3,4,5,6,7,8,9,10,11,15]\n",
    "features_user_2 = [0,1,2,3,4,5,16,17,18,19,20,21,22]\n",
    "print(dataset.columns[[0,1,2,3,4,5,6,7,8,9,10,11,15]])\n",
    "print(dataset.columns[[0,1,2,3,4,5,16,17,18,19,20,21,22]])\n",
    "\n",
    "print(dataset.iloc[:, features_user_1])\n",
    "print(dataset.iloc[:, features_user_2])\n",
    "user_1 = dataset.iloc[:, features_user_1]\n",
    "user_2 = dataset.iloc[:, features_user_2]\n",
    "\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_1_dist = user_1[\"distance_1\"]\n",
    "user_1_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.apply(pd.to_numeric)\n",
    "\n",
    "\n",
    "\n",
    "# The current split is 95% of data is used for training and 5% for validation of the model\n",
    "train = dataset.sample(frac=0.95,random_state=200)\n",
    "test = dataset.drop(train.index)\n",
    "#train = train.as_matrix()\n",
    "#test = test.as_matrix()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#train_loader = torch.utils.data.DataLoader(dataset=train, batch_size=batch_size, shuffle=True)\n",
    "#test_loader = torch.utils.data.DataLoader(dataset=test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Option 1: Loading the dataset, we have no output labels in the classical sense\n",
    "#train = torch.utils.data.TensorDataset(torch.Tensor(train).float())\n",
    "#test = torch.utils.data.TensorDataset(torch.Tensor(test).float())\n",
    "# Option 2: Custom DataSet\n",
    "train = gjnn.dataloader.Dataset(train) \n",
    "test = gjnn.dataloader.Dataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "n_iters = 50000\n",
    "num_epochs = n_iters / (len(train) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "print(\"The number of epochs is: \" + str(num_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"After data loading\")\n",
    "\n",
    "# Setting other neural network hyperparameters\n",
    "hidden_layer_size = 20\n",
    "siamese_layer_size = 20\n",
    "output_layer_size = 1\n",
    "num_features_per_branch = 13\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "num_epoch = 5\n",
    "\n",
    "\n",
    "\n",
    "# Check dimensions of features\n",
    "model = gjnn.model.SiameseNetwork(num_features_per_branch, siamese_layer_size, hidden_layer_size, output_layer_size)\n",
    "print(\"Model correctly initialized...\")\n",
    "\n",
    "# Initialization of the Loss Function\n",
    "criterion = gjnn.loss.DistanceLoss()\n",
    "print(\"Distance Loss Correctly Initialized...\")\n",
    "\n",
    "# At the moment we stick to a classic SGD algorithm, maybe we can change it to Adam\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "print(\"Optimizer Instantiated...\")\n",
    "\n",
    "\n",
    "iter = 0\n",
    "\n",
    "# TEST ON A BATCH OF THE DATASET\n",
    "for i, (user1, user2, user1_dist, user2_dist) in enumerate(train_loader):\n",
    "    print(i)\n",
    "    print(user1, user2, user1_dist, user2_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch \" + str(epoch))\n",
    "    print(train_loader)\n",
    "    for i, (user_1, user_2, user_1_dist, user_2_dist) in enumerate(train_loader):\n",
    "        #features_u1 = Variable(user_1.view(-1, num_features))\n",
    "        #features_u2 = Variable(user_2.view(-1, num_features))\n",
    "        features_u1 = Variable(user_1)\n",
    "        features_u2 = Variable(user_2)\n",
    "        dist_u1 = Variable(user_1_dist)\n",
    "        dist_u2 = Variable(user_2_dist)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Here we have to give data which goes to branch 1 and data who goes on branch 2\n",
    "        outputs = model(features_u1, features_u2)\n",
    "        \n",
    "        loss = criterion(user_1_dist, user_2_dist, outputs)\n",
    "        losses.append(loss)\n",
    "        print(\"loss for i {} is equal to: {}\".format(i, loss))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        print(iter)\n",
    "        # we want to check the accuracy with test dataset every 500 iterations\n",
    "        # we can change this number, it is just if it is too small we lose a lot of time\n",
    "        # checking accuracy while if it is big, we have less answers but takes less time for the algorithm\n",
    "        #if iter % 500 == 0:\n",
    "            # calculate accuracy\n",
    "        #    correct = 0\n",
    "        #    total = 0\n",
    "            \n",
    "            # iterate through test dataset\n",
    "         #   for features, labels in test_loader:\n",
    "         #       features = Variable(features.view(-1, num_features))\n",
    "                \n",
    "         #      outputs = model(features)\n",
    "                # get predictions from the maximum value\n",
    "         #       _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # total number of labels\n",
    "         #       total += labels.size(0)\n",
    "                \n",
    "         #       correct += (predicted == labels).sum()\n",
    "            \n",
    "         #   accuracy = 100 * correct / total\n",
    "            \n",
    "         #   print(\"Iteration: {}. Loss: {}. Accuracy: {}\".format(iter, loss.data[0], accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in losses:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
